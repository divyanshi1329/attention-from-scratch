# Attention Mechanisms From Scratch (PyTorch)

This project implements attention mechanisms step-by-step from first principles,
without using high-level PyTorch attention APIs.

## What is implemented
- Dot-product attention
- Full self-attention matrix
- Scaled dot-product attention
- Trainable QKV self-attention
- Causal (masked) self-attention
- Multi-head attention
- Attention weight heatmap visualization

All implementations are written from scratch for learning purposes.

## Notebook
- `attention-from-scratch.ipynb`

## How to run
```bash
pip install torch matplotlib
jupyter notebook

